<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Anderson R da Silva" />

<meta name="date" content="2025-04-02" />

<title>biotools: an overview</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">biotools: an overview</h1>
<h4 class="author">Anderson R da Silva</h4>
<h4 class="date">2025-04-02</h4>



<div id="about" class="section level1">
<h1>About</h1>
<p><em>biotools</em> has implementations to perform and work with
cluster analysis, especially Tocher’s method, and tools for evaluating
clustering outcomes, such as a specific coefficient of cophenetic
correlation, discriminant analysis, the Box’M test and the Mantel’s
permutation test. A new approach for calculating the power of Mantel’s
test is implemented. Some of those are illustrated in the next
sections.</p>
<p>Target audience: agronomists, biologists and researchers of related
fields.</p>
<div id="installation" class="section level2">
<h2>Installation</h2>
<p>You can install the released version on CRAN or the beta version from
GitHub:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;biotools&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>devtools<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&quot;arsilva87/biotools&quot;</span>)</span></code></pre></div>
<p>Load it:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(biotools)</span></code></pre></div>
<pre><code>## Loading required package: MASS</code></pre>
<pre><code>## ---
## biotools version 4.3</code></pre>
</div>
</div>
<div id="multivariate-pairwise-comparisons" class="section level1">
<h1>Multivariate pairwise comparisons</h1>
<p>Take the data set <code>maize</code> from biotools. They consist of
multivariate observations on ears of five maize genotypes
(families).</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(maize)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(maize)   <span class="co"># firts 6 rows</span></span></code></pre></div>
<pre><code>##       NKPR       ED       CD       PH family env
## 1 36.02031 4.333750 2.317500 2.087813      1   1
## 2 34.24688 4.045000 2.251250 1.892187      2   1
## 3 28.12656 3.741250 2.141250 2.119375      3   1
## 4 30.72500 4.321875 2.288125 2.085000      4   1
## 5 34.00625 4.660000 2.500625 2.100312      5   1
## 6 36.50000 4.328750 2.436250 2.196875      1   2</code></pre>
<p>Let us fit a MANOVA model and test for statistical significance of
‘family’ using Wilks’ lambda.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">cbind</span>(NKPR, ED, CD) <span class="sc">~</span> family, <span class="at">data =</span> maize)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(M, <span class="at">test =</span> <span class="st">&quot;Wilks&quot;</span>)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
##             Df    Wilks approx F num Df den Df  Pr(&gt;F)    
## (Intercept)  1 0.001322   3272.8      3 13.000 &lt; 2e-16 ***
## family       4 0.100255      4.0     12 34.686 0.00066 ***
## Residuals   15                                            
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Now, one question is <strong>where</strong> are those differences? To
answer that, multiple pairwise tests can be run. The function
<code>mvpaircomp()</code> allows on to choose between multivariate
statistics such as Wilks’ lambda, Pillai’s trace etc. to perform
multiple tests on factor levels a fitted model.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mvpaircomp</span>(M, <span class="st">&quot;family&quot;</span>, <span class="at">test =</span> <span class="st">&quot;Wilks&quot;</span>, <span class="at">adjust =</span> <span class="st">&quot;bonferroni&quot;</span>)</span></code></pre></div>
<pre><code>## 
##             Multivariate Pairwise Comparisons
## 
##         Wilks approx F num DF den DF   Pr(&gt;F)   
## 1 - 2 0.57758   3.1693      3     13 0.604689   
## 1 - 3 0.32690   8.9225      3     13 0.017946 * 
## 1 - 4 0.55591   3.4616      3     13 0.480571   
## 1 - 5 0.67364   2.0994      3     13 1.000000   
## 2 - 3 0.61192   2.7482      3     13 0.853092   
## 2 - 4 0.60398   2.8413      3     13 0.789525   
## 2 - 5 0.46341   5.0176      3     13 0.158342   
## 3 - 4 0.31965   9.2230      3     13 0.015581 * 
## 3 - 5 0.24296  13.5019      3     13 0.002738 **
## 4 - 5 0.80370   1.0584      3     13 1.000000   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## With bonferroni p-value adjustment for multiple comparisons</code></pre>
<div id="boxs-m-test" class="section level2">
<h2>Box’s M-test</h2>
<p>One requirement of the model fitted earlier is that is residual
covariance matrices of the factor levels are homogeneous. Furthermore,
in cluster analysis, it might be of interest to check if the covariance
matrices of the clusters can be considered equals, especially if one
intends to perform a linear discriminant analysis using a pooled
matrix.In those cases, the Box’s M-test can be applied. The function
<code>boxM()</code> performs the test using an approximation of the
<span class="math inline">\(\chi^2_\nu\)</span> distribution, where
<span class="math inline">\(\nu = \frac{p(p+1)(k-1)}{2}\)</span>, <span class="math inline">\(p\)</span> is the number of variables and <span class="math inline">\(k\)</span> is the number of levels.</p>
<p>Users should be aware that all clusters must have a positive definite
covariance matrix. If there is any cluster/level containing fewer
observations (rows) than the number of variables (columns), then its
covariance matrix is probably not positive definite.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">residuals</span>(M)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">boxM</span>(<span class="at">data =</span> res, <span class="at">grouping =</span> maize<span class="sc">$</span>family)</span></code></pre></div>
<pre><code>## 
##  Box&#39;s M-test for Homogeneity of Covariance Matrices
## 
## data:  res
## Chi-Sq (approx.) = 32.517, df = 24, p-value = 0.1147</code></pre>
</div>
</div>
<div id="importance-of-variables-the-singhs-criterion" class="section level1">
<h1>Importance of variables: the Singh’s criterion</h1>
<p>The function singh() runs the method proposed by Singh (1981) for
determining the importance of variables based on the squared generalized
Mahalanobis distance. In his approach, the importance of the <span class="math inline">\(j\)</span>-th variable (<span class="math inline">\(j = 1, 2, ..., p\)</span>) on the calculation of
the distance matrix can be obtained by:</p>
<p><span class="math display">\[
    S_{.j} = \sum_{i=1}^{n-1} \sum_{i&#39;&gt;i}^{n} (x_{ij} -
x_{i&#39;j}) ({\bf x}_i - {\bf x}_{i&#39;})^T {\Sigma}_{.j}^{-1}
\]</span></p>
<p>where <span class="math inline">\(x_{ij}\)</span> and <span class="math inline">\(x_{i&#39;j}\)</span> are the observation taken at
the <span class="math inline">\(i\)</span>-th and <span class="math inline">\(i&#39;\)</span>-th objects (individuals) for the
<span class="math inline">\(j\)</span>-th variable, <span class="math inline">\({\bf x}_i\)</span> is the <span class="math inline">\(p\)</span>-variate vector of the <span class="math inline">\(i\)</span>-th object and <span class="math inline">\({\Sigma}_{.j}^{-1}\)</span> is the <span class="math inline">\(j\)</span>-th column of the inverse of the
covariance matrix.</p>
<p>Since <span class="math inline">\(S_{.j}\)</span> is itself a measure
of distance, it can be more appropriate to take the following proportion
instead:</p>
<p><span class="math display">\[
        \frac{S_{.j}} {\sum_{j=1}^{p} S_{.j}} \in [0, 1]
\]</span></p>
<p>with the constraint <span class="math inline">\(\sum_{j=1}^{p} S_{.j}
= \sum_{i=1}^{n-1} \sum_{i&#39;&gt;i}^{n} D_{ii&#39;}^2\)</span>.</p>
<p>Using the proportion enable us to determine the relative importance
of each variable.</p>
<div id="example" class="section level3">
<h3>Example</h3>
<p>Using the residual covariance of the fitted MANOVA model, let us
calculate the importance of the three variables to discriminating
observations.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">singh</span>(<span class="at">data =</span> maize[, <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>], <span class="at">cov =</span> <span class="fu">cov</span>(res))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>s</span></code></pre></div>
<pre><code>##                                 ED        NKPR           CD
## Singh statistic       2292.2558745 485.7898274 187.46962697
## Proportion               0.7729705   0.1638130   0.06321654
## Cumulative proportion    0.7729705   0.9367835   1.00000000
## attr(,&quot;class&quot;)
## [1] &quot;singh&quot;</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(s)</span></code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
</div>
</div>
<div id="tochers-clustering" class="section level1">
<h1>Tocher’s clustering</h1>
<p>Cluster analysis consists of arranging multivariate observations into
homogeneous clusters. There are several algorithms for cluster analysis,
with different outcomes and objective functions. Tocher’s optimization
method allows one to establish mutually exclusive clusters, with no need
to define the number of clusters. It has been widely used in studies of
genetic/phenotypic diversity that are based on cluster analysis.
Furthermore, Tocher’s method can be used to determine the number of
clusters in dendrograms.</p>
<p>Clusters are established according to an objective function that
adopts an optimization criterion, which minimizes the average
intra-cluster distance and maximizes the average inter-cluster distances
(Silva &amp; Dias, 2013). <strong>biotools</strong> contains the method
suggested by K.D. Tocher (Rao, 1952) for clustering objects, based on
the algorithm:</p>
<ol style="list-style-type: decimal">
<li>(Input) Take a distance matrix, let us say <span class="math inline">\({\bf d}\)</span>, of size <span class="math inline">\(n\)</span>.</li>
<li>Define a clustering criterion, that is, a limit of distance at which
to evaluate the inclusion of objects in a cluster in formation. Usually,
this criterion is defined as follows: let <span class="math inline">\({\bf m}\)</span> be a vector of size <span class="math inline">\(n\)</span> containing the smallest distances
involving each object, then take <span class="math inline">\(\theta =
\max{({\bf m})}\)</span> to be the clustering criterion.</li>
<li>Locate the pair of objects with the smallest distance in <span class="math inline">\({\bf d}\)</span> to be the starting cluster.</li>
<li>Compute the average distance (<span class="math inline">\(d_{k(j)}\)</span>) of the starting cluster, say
<span class="math inline">\(k\)</span>, to a certain object, say <span class="math inline">\(j\)</span>, that shows the smallest pairwise
distance with each object in the starting cluster.</li>
<li>Compare the statistic evaluated at the step 4 with <span class="math inline">\(\theta\)</span>. Then, add the new object to the
starting cluster if <span class="math inline">\(d_{k(j)} \leq
\theta\)</span>. Otherwise, go to step 3 not considering the objects
already clustered and start a new cluster.</li>
</ol>
<p>The process continues until the last remaining object is evaluated
and either included in the last cluster formed or allocated to a new
cluster. The function <code>tocher()</code> performs optimization
clustering and returns an object of class <code>tocher</code>, which
contains the clusters formed, a numeric vector indicating the cluster of
each object, a matrix of cluster distances and also the input - a class
<code>dist</code> object.</p>
<div id="example-1" class="section level3">
<h3>Example</h3>
<p>The 20 individuals (observation) from the maize data set are to be
clustered. First, we need to comput the Mahalanobis generalized squared
distance among them.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">D2.dist</span>(<span class="at">data =</span> maize[, <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>], <span class="at">cov =</span> <span class="fu">cov</span>(res))</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">range</span>(d)</span></code></pre></div>
<pre><code>## [1]  0.2203346 61.4121118</code></pre>
<p>Then Tocher’s method can be applied to determine clusters. For that
we are using modified method (sequential algorithm) by Vasconcelos et
al. (2007) and the residual covariance matrix.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>toc <span class="ot">&lt;-</span> <span class="fu">tocher</span>(d, <span class="at">algorithm =</span> <span class="st">&quot;sequential&quot;</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>toc</span></code></pre></div>
<pre><code>## 
##           Tocher&#39;s Clustering 
## 
## Call: tocher.dist(d = d, algorithm = &quot;sequential&quot;)
## 
## Cluster algorithm: sequential 
## Number of objects: 20 
## Number of clusters: 6 
## Most contrasting clusters: cluster 5 and cluster 6, with 
##    average intercluster distance: 42.88234
## 
## $`cluster 1`
## [1] 6  16 12 10
## 
## $`cluster 2`
## [1] 7  13 2  8 
## 
## $`cluster 3`
## [1] 15 20 4  5 
## 
## $`cluster 4`
## [1] 9  19 3  17
## 
## $`cluster 5`
## [1] 1  11 14
## 
## $`cluster 6`
## [1] 18</code></pre>
</div>
<div id="cluster-distances" class="section level2">
<h2>Cluster distances</h2>
<p>After obtaining the clusters, it might be useful to know how
divergent they are from each other. In this context, cluster distances
are calculated from the original distance matrix through the function
<code>distClust()</code>. An intracluster distance is calculated by
averaging all pairwise distances among objects in the cluster concerned.
Likewise, the distance between two clusters is calculated by averaging
all pairwise distances among objects in these clusters.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(toc<span class="sc">$</span>distClust, <span class="at">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##           cluster 1 cluster 2 cluster 3 cluster 4 cluster 5 cluster 6
## cluster 1       3.6      14.3      11.8      15.4       9.9      29.6
## cluster 2      14.3       3.7      27.7      13.9      25.4       7.1
## cluster 3      11.8      27.7       3.5      15.1       7.5      41.1
## cluster 4      15.4      13.9      15.1       4.8      21.1      13.4
## cluster 5       9.9      25.4       7.5      21.1      12.6      42.9
## cluster 6      29.6       7.1      41.1      13.4      42.9       0.0</code></pre>
</div>
<div id="cophenetic-correlation" class="section level2">
<h2>Cophenetic correlation</h2>
<p>Clustering validation is widely applied for hierarchical and
iterative methods. Some measures of internal validation for a Tocher’s
clustering outcome are implemented on biotools.</p>
<p>The approach presented by Silva (2013) is implemented by taking the
cluster distances in order to build a cophenetic matrix for clustering
performed through the Tocher’s method. Their approach consists of taking
the cophenetic distance among objects located in the same cluster as the
intracluster distance and the cophenetic distance between objects of
different clusters as the intercluster distance. Then, the Pearson’s
correlation between the elements of the original and cophenetic matrix
can be taken as a cophenetic correlation. The function to be called is
<code>cophenetic()</code>, whose input is an object of class
<code>tocher</code> and its output an object of class
<code>dist</code>.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>cop <span class="ot">&lt;-</span> <span class="fu">cophenetic</span>(toc)  <span class="co"># cophenetic matrix</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(d, cop)             <span class="co"># cophenetic correlation coefficient</span></span></code></pre></div>
<pre><code>## [1] 0.7320551</code></pre>
</div>
<div id="mantels-test" class="section level2">
<h2>Mantel’s test</h2>
<p><strong>Is that cophenetic correlation significantly greater than
zero?</strong> That quastion and others related to square matrix
associations can be answered using Mantel’s permutation test.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mantelTest</span>(d, cop, <span class="at">nperm =</span> <span class="dv">900</span>)</span></code></pre></div>
<p><img src="data:image/png;base64,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" /><!-- --></p>
<pre><code>## 
##             Mantel&#39;s permutation test
## 
## Correlation: 0.7320551
## p-value: 0.001109878, based on 900 matrix permutations
## Alternative hypothesis: true correlation is greater than 0</code></pre>
</div>
</div>
<div id="discriminant-analysis-based-on-mahalanobis-distance" class="section level1">
<h1>Discriminant analysis based on Mahalanobis distance</h1>
<p>A simple and efficient object classification rule is based on
Mahalanobis distance. It consists of calculating the squared generalized
Mahalanobis distances of each multivariate observation to the centre of
each cluster. biotools performs this sort of discriminant analysis
through <code>D2.disc()</code>, as follows:</p>
<p>Consider the <span class="math inline">\(i\)</span>-th (<span class="math inline">\(i = 1, 2, ..., n_j\)</span>) <span class="math inline">\(p\)</span>-variate observation belonging to the
<span class="math inline">\(j\)</span>-th (<span class="math inline">\(j
= 1, 2, ..., k\)</span>) cluster, <span class="math inline">\({\bf
x}_{ij}\)</span>. Let <span class="math inline">\(\bar{{\bf
x}}_{j&#39;}\)</span> be the vector of means of the <span class="math inline">\(j&#39;\)</span>-th (<span class="math inline">\(j&#39; = 1, 2, ..., k\)</span>) cluster. The
Mahalanobis distance from this observation to the centre of this cluster
is given by</p>
<p><span class="math display">\[
    D_{ij, j&#39;}^2 = ({\bf x}_{ij} - \bar{{\bf x}}_{j&#39;})^T
{\hat{\Sigma}}_{pooled}^{-1} ({\bf x}_{ij} - \bar{{\bf x}}_{j&#39;})
\]</span></p>
<p>where <span class="math inline">\({\hat{\Sigma}}_{pooled}\)</span> is
the estimate of the pooled covariance matrix for clusters.</p>
<p>Now consider <span class="math inline">\(C_j\)</span> the random
variable that represents the cluster at which the observation <span class="math inline">\({\bf x}_{ij}\)</span> lies. The predicted class
<span class="math inline">\(\hat{C}_{j&#39;}\)</span> for this
observation is the one such that</p>
<p><span class="math display">\[
    j&#39; \Rightarrow \min_{j&#39; = 1}^{k} ( D_{ij, j&#39;}^2 )
\]</span></p>
<p>i.e., the object is allocated to the cluster whose distance from its
centre is the smallest.</p>
<p>The output is the Mahalanobis distances from each observation to the
centre of each cluster, the pooled covariance matrix for clusters and
the confusion matrix, which contains the number of correct
classifications in the diagonal cells and misclassifications
off-diagonal. In addition, a column misclass indicates (with an
asterisk) where there was disagreement between the original
classification (grouping) and the predicted one (pred).</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">D2.disc</span>(<span class="at">data =</span> maize[, <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>], </span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">grouping =</span> maize<span class="sc">$</span>family, </span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">pooled.cov =</span> <span class="fu">cov</span>(res))</span></code></pre></div>
<pre><code>## 
## Call:
## D2.disc.default(data = maize[, 1:3], grouping = maize$family, 
##     pooled.cov = cov(res))
## 
## Mahalanobis distances from each class and class prediction (first 6 rows):
##           X1        X2        X3        X4        X5 grouping pred misclass
## 1  0.6075575  9.712976 20.970574  9.584311  5.870589        1    1         
## 2  3.6521169  3.202266  6.936814 10.989061 12.524312        2    2         
## 3 16.5825884  4.874949  3.468493 10.094606 19.106220        3    3         
## 4 10.7383299 16.714143 31.501871  4.121057  3.496142        4    5        *
## 5 11.9533308 22.662364 47.834072  7.030199  2.268491        5    5         
## 6  1.7343324  4.486490 17.479342  8.732280  7.262779        1    1         
## 
## Class means:
##       NKPR       ED       CD
## 1 36.10502 4.349705 2.369759
## 2 32.27656 4.019219 2.321406
## 3 30.05977 3.639063 2.127031
## 4 30.99883 4.243906 2.367656
## 5 33.70898 4.458906 2.425625
## 
## Confusion matrix:
##   new 1 new 2 new 3 new 4 new 5
## 1     4     0     0     0     0
## 2     0     3     1     0     0
## 3     0     0     4     0     0
## 4     0     1     0     1     2
## 5     0     0     0     0     4</code></pre>
</div>
<div id="miscellanea" class="section level1">
<h1>Miscellanea</h1>
<p><em>biotools</em> also contains several other useful miscelaneous
tools, such as the statistical tests for genetic covariance components,
the exact test for seed lot heterogeneity, an approach for predicting
spatial gene diversity and a function to perform path analysis dealing
with collinearity.</p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>Mantel, N. 1967. The detection of disease clustering and a
generalized regression approach. Cancer Research 27:209–220.</p>
<p>Rao, C. R. 1952. Advanced statistical methods in biometric research.
New York: John Wiley &amp; Sons.</p>
<p>Silva, A. R., and C. T. S. Dias. 2013. A cophenetic correlation
coefficient for Tocher’s method. Pesquisa Agropecuaria Brasileira 48:
589–96.</p>
<p>Singh, D. 1981. The relative importance of characters affecting
genetic divergence. Indian Journal Genetics and Plant Breeding 41:
237-45.</p>
<p>Vasconcelos, E.S., Cruz, C.D., Bhering, L.L., Resende-Jr, M.F.R.
2007. Metodo Alternativo para Analise de Agrupamento. Pesquisa
Agropecuaria Brasileira 42:1421-1428.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
